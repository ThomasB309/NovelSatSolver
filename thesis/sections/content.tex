%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.5, 2020-06-26

\chapter{Preliminaries}
\label{ch:Preliminaries}

This chapter contains the preliminaries, that are necessary, in order to understand the SAT solving problem of this thesis. Most of the definitions were taken out of the "Handbook of Satisfiability: Second Edition" \cite{biere2009handbook}.

\section{The Boolean Satisfiability Problem}
Let q be a Boolean formula, that contains a set of literals $\{x_1,...,x_n\}$. A literal is either the variable or the negation of the variable. The satisfiability problem is concerned with, whether there exists a set of variable assignments $\{x_1,...x_n\}$, that satisfy the formula q. The problem is solved, if either a satisfying assignment is found, or if the program decides that the formula is not satisfiable. \cite{biere2009handbook}

\begin{example}
\begin{leftbar}
\begin{displaymath}
x_1 \vee \neg x_2 \vee x_3
\end{displaymath}
This formula is satisfiable with the following possible variable assignments:
\begin{displaymath}
\{1,1,0\},\{0,0,0\},\{0,1,1\},\{1,0,0\},\{0,0,1\},\{1,1,1\},\{1,0,1\}
\end{displaymath}
\end{leftbar}
\caption{Example of a satisfiable Boolean formula}
\label{ex:SatFormula}
\end{example}

The Example \ref{ex:SatFormula} shows a formula, that is satisfiable with several possible sets of variable assignments.

\begin{example}
\begin{leftbar}
\begin{displaymath}
x_1 \wedge \neg x_1 \wedge x_2
\end{displaymath}
There exists not possible assignment that satisfies the given formula
\end{leftbar}
\caption{Example of an unsatisfiable Boolean formula}
\label{ex:UnsatFormula}
\end{example}

The Example \ref{ex:UnsatFormula} shows a formula, that can't be satisfied, no matter how the variables are assigned. This is due to the conflicting conjunction $x_1 \wedge \neg x_1$. This conjunction forces $x_1$ to both be false and true and is therefore not satisfiable.

\section{The Conjunctive Normal Form (CNF)}

There are many ways, how a Boolean satisfiability problem can be expressed.

\begin{example}
\begin{leftbar}
The following two formulas are equivalent:
\begin{displaymath}
(x_1 \implies x_2) \iff (\neg x_1 \vee x_2)
\end{displaymath}
\end{leftbar}
\caption{Example of two equivalent Boolean formulas}
\label{ex:EqFormulas}
\end{example}

The Example \ref{ex:EqFormulas} shows the two equivalent formulas $x_1 \implies x_2$ and $\neg x_1 \vee x_2$. Because these two formulas are equivalent, the Boolean satisfiability problem also has the same solution for both of these formulas. Even though both formulas are semantically equivalent, it doesn't necessarily mean that the solving process of the SAT problem has the same complexity.

Historically most SAT solver algorithms were designed, in order to solve formulas in their "conjunctive normal form" (CNF) \cite{biere2009handbook}. A CNF consists of a set of clauses, that are conjunctively combined using the $\vee$ operator. A clause is a set of literals which are disjunctively combined. \cite{biere2009handbook}

This form was chosen, because it has the advantage of being very simple \cite{biere2009handbook}. All clauses need to be satisfied, in order for the whole formula to be satisfied and a clause is satisfied, if at least one of its literals is satisfied. This simple form makes the implementation of algorithms easier and enables a common file format \cite{biere2009handbook}.

\section{The DIMACS File Format}

The SAT solver community uses a common file format, which is called the "DIMACS file format", and was first introduced in the DIMACS Challenge of 1993 \cite{johnson1996cliques}. A common file format allows newly implemented SAT solvers to use standardized benchmarks, in order to assess their performance \cite{biere2009handbook}.

\begin{center}
\begin{leftbar}
\begin{tabular}{l}
\texttt{c SAT benchmark} \\
\texttt{p cnf 3 3} \\
\texttt{1 2 3 0} \\
\texttt{-1 3 0} \\
\texttt{1 -2 0}\\
\end{tabular}
\end{leftbar}
\captionof{example}{Example of an input file in the DIMACS file format}
\label{ex:DIMACS}
\end{center}
The Example \ref{ex:DIMACS} shows a possible input file in the DIMACS format. The input starts with the comment "SAT benchmark". This comment is denoted by the letter "c". Every line that starts with a "c" has to be ignored by the SAT solver. Before the CNF can be defined, there needs to be a preamble, that is denoted by the phrase "p cnf", followed by the number of variables and clauses in the CNF. In this case the formula has three variables and three clauses. After the preamble the formula can be defined. Each line contains exactly one clause, which is escaped by the character "0". \cite{biere2009handbook}

\section{The Davis–Putnam–Logemann–Loveland (DPLL) Algorithm}
\label{sec:dpll}

\begin{algorithm}
\caption{DPLL(CNF formula) \cite{biere2009handbook}}\label{alg:DPLL}
\begin{algorithmic}
\State $(I,G) = UNIT-RESOLUTION(formula)$
\If{$G = \{\}$}
	\State return I
\ElsIf{$\{\} \in G$}
    \State return UNSATISFIABLE
\Else
	\State choose a literal L in G
	\If{$L = DPLL(G|L) \neq UNSATISFIABLE$}
		\State $return \; L \cup I \cup \{L\}$
	\ElsIf{$L = DPLL(G|\neg L) \neq UNSATISFIABLE$}
		\State $return \; L \cup I \cup \{\neg L\}$
	\Else
		\State return UNSATISFIABLE
	\EndIf
\EndIf
\end{algorithmic}
\end{algorithm}


The pseudo code from algorithm \ref{alg:DPLL} \cite{biere2009handbook}, shows the DPLL-algorithm, which is a depth-first search algorithm in the space of possible variable assignments of a formula. The DPLL algorithm was first introduced in the paper "A machine program for theorem proving" \cite{davis1962machine} by Davis, Logemann and Loveland. In contrast to other algorithms, like for example the resolution algorithm, the DPLL algorithm doesn't try to prove the satisfiability of a formula by using boolean algebra \cite{biere2009handbook}. It is just an exhaustive search of every possible truth assignment. When the algorithm finds an assignment, that satisfies the formula, then the satisfiability is proven. If the algorithm can't find any assignment that satisfies the formula, then it is unsatisfiable \cite{biere2009handbook}.

\begin{figure}[!htb]
\begin{leftbar}
Consider the formula $x_1 \wedge \neg x_2 \wedge x_3$. The search tree then looks as follows:\\
\centering
\begin{tikzpicture}
\Tree [.$x_1$ \edge node[auto=right] {1}; [.$x_2$ \edge node[auto=right] {1}; [.$x_3$ \edge node[auto=right] {1}; [.false ] \edge node[auto=left] {0}; [.false ] ] \edge node[auto=left] {0}; [.$x_3$ \edge node[auto=right] {1}; [.true ] \edge node[auto=left] {0}; [.false ] ] ] \edge node[auto=left] {0}; [.$x_2$  \edge node[auto=right] {1}; [.$x_3$ \edge node[auto=right] {1}; [.false ] \edge node[auto=left] {0}; [.false ] ] \edge node[auto=left] {0}; [.$x_3$ \edge node[auto=right] {1}; [.false ] \edge node[auto=left] {0}; [.false ] ]]]
\end{tikzpicture}
\end{leftbar}
\caption{Example for a search tree in the DPLL algorithm}
\label{ex:SearchTree}
\end{figure}

The tree in Example \ref{ex:SearchTree} shows all possible variable assignments for the formula $x_1 \wedge \neg x_2 \wedge x_3$. During each iteration the DPLL algorithm chooses a variable of the formula and assigns a value to it. Which variable is chosen and what value gets assigned, can differ depending on which branching heuristic is used. The time, that the algorithm needs, in order to come to a conclusion, can vary greatly depending on which branching heuristic is used. If the formula is false after an assignment and the algorithm hasn't tried every possible assignment, then it backtracks and tries another combination. If the search space is already exhausted, then the formula is not satisfiable. In the example the algorithm would stop as soon as it finds the assignment $\{1,0,1\}$, because the formula is satisfied.

\section{Unit Resolution}
One very important aspect of the DPLL algorithm is the unit resolution step. For this step it is important to understand what the term "unit literal" means in the context of a solving algorithm. A unit literal is a literal, that has to be true, in order to satisfy the formula. Such a unit literal can either occur, because the formula contains a clause with a single literal, or if the formula contains a clause with a single non false literal under the current partial assignment. The DPLL algorithm uses these unit literals to instantly give the variables an assignment. \cite{biere2009handbook}

\begin{example}
\begin{leftbar}
Consider the clause $x_1 \vee \neg x_2 \vee x_3$ and the partial variable assignment $\{x_1=0,x_3=0\}$. As we already know, all clauses of a CNF need to be satisfied, in order for the formula to be satisfied. Because $x_1$ and $x_3$ are already false, only $\neg x_2$ can still satisfy the clause. This makes $\neg x_2$ a unit literal.
\end{leftbar}
\caption{Example for a unit propagation}
\label{ex:Unitliteral}
\end{example}

The Example \ref{ex:Unitliteral} shows a situation, where a clause contains a unit literal as a result of the partial assignment of the DPLL algorithm. Because $\neg x_2$ is a unit literal the DPLL algorithm can instantly set the variable $x_2$ to false, which reduces the search space.

\section{Resolvent}
In order to understand how the DPLL algorithm evolved into the more advanced CDCL algorithm, it is important to know what the "resolvent" of two clauses is. The specific rule, that is used in todays modern SAT solvers was introduced in the paper "A machine-oriented logic based on the resolution principle" \cite{robinson1965machine} by Robinson.

\begin{definition}
\begin{leftbar}
Let A and B be two clauses of a Boolean formula and $x$ a variable. The literal x is contained in A and the literal $\neg x$ is contained in B. Then the resolvent of the two clauses is defined in the following way:
\begin{displaymath}
(A - x) \cup (B - \neg x)
\end{displaymath}
\end{leftbar}
\caption{Definition of the resolution rule \cite{robinson1965machine}}
\label{def:Resolution}
\end{definition}

The Definition \ref{def:Resolution} shows, how the resolvent of two clauses is defined. An important aspect of the resolvent is, that it is always implied by the clauses, that are resolved by it \cite{biere2009handbook}. As already explained, a boolean formula in its CNF form can only be true, if every clause is true. In a satisfying assignment the variable $x$ is either true or false. If x is false, then $(A - x)$ has to be true, in order for A to be true. If x is true, then $(B - \neg x)$ has to be true, in order for B to be true. Therefore $(A - x) \cup (B - \neg x)$ is implied.

\begin{leftbar}
Consider the two clauses $x_1 \vee x_2 \vee x_3$ and $x_4 \vee x_5 \vee \neg x_3$. Then the resolvent of both clauses is:
\begin{displaymath}
x_1 \vee x_2 \vee x_4 \vee x_5
\end{displaymath}
\end{leftbar}

\section{The Conflict-Driven-Clause-Learning (CDCL) Algorithm}
\label{sec:cdcl}

\begin{algorithm}
\caption{CDCL(F) \cite{biere2009handbook}}\label{alg:CDCL}
\begin{algorithmic}
\State $DLevel \gets 0$
\If{!UnitPropagation()}
	\State return false
\EndIf

\While{!AllVariablesAssigned()}
	\State $DLevel \gets DLevel + 1$
	\State $(var, val) \gets PickBranchVariable()$
	\State $Assign(var, val DLevel, \delta)$
	
	\While{!UnitPropagation()}
		\If {DLevel == 0}
			\State return false
		\EndIf
	
	
		\State $BLevel \gets ConflictAnalysis()$
		\State $Backtrack(BLevel)$
		\State $DLevel \gets BLevel$
		\If {$TimeToRestart()$}
			\State $Backtrack(0)$
			\State $DLevel \gets 0$
		\EndIf
	\EndWhile
\EndWhile

\State return true

\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:CDCL} \cite{biere2009handbook} shows the pseudo code for the CDCL algorithm. Because the DPLL algorithm is just a simple exhaustive depth-first search in the space of possible variable assignments, it can take a really long time to find a solution for large formulas. In order to increase the solving speed, many optimizations are used. An example for that is the unit propagation step, which limits the search space of the algorithm significantly \cite{biere2009handbook}. This section explains a variation of the DPLL algorithm which is called the "Conflict-Diven-Clause-Learning algorithm". This algorithm combines the depth-first search approach of the DPLL algorithm, with the inference based learning techniques, that use the resolvent of clauses. \cite{biere2009handbook}

The DPLL algorithm uses a technique called "chronological backtracking". If the algorithm finds a contradicting assignment, it backtracks to the last decision level, and switches the branching decision. This is repeated until a satisfying assingment is found, or the algorithm can't backtrack any further, because all combinations are exhausted. This results in the formula not being satisfiable. \cite{biere2009handbook}

The CDCL algorithm tries to make use of the information, that can be extracted from a contradiction, in order to allow for "non-chronological backtracking". This basically means that we can backtrack further than just one level backwards. \cite{biere2009handbook}
Such a contradiction between two clauses can only arise, when one of the clauses has a literal $x$ and another clause has the literal $\neg x$. If all other literals in both clauses are false, then $x$ and $\neg x$ have to both be true, in order to satisfy both clauses. This causes a conflict, which can be resolved by adding the resolvent of the two clauses to the formula. The resolvent of two clauses is always implied by the resolved clauses and therefore also implied by the whole formula, which is why it can be added to the formula without changing the solution of the Boolean satisfiability problem. By adding the resolvent of two conflicting clauses to the formula, the solver can prevent a partial assignment, that leads to the conflict. This process is called clause learning. \cite{biere2009handbook}

\section{Two-Watched-Literals Algorithm}
\label{sec:twoWatchedLiterals}

The "Two-Watched-Literals Algorithm" as described in "Chaff: Engineering an Efficient SAT Solver" \cite{moskewicz2001chaff} by Moskewicz et al. is an efficient algorithm for implementing the unit propagation of a clause based SAT Solver. To understand the efficiency of this algorithm, we can take a look at how the propagation of literals can be implemented in a naive way. Theoretically you could iterate over every clause in the database and check, if the next variable assignment forces a unit propagation in a clause. This is inefficient, because not all clauses need to be visited for every variable assignment. If a literal in clause turns true, then the clause is satisfied and can't force a unit propagation anymore. It therefore doesn't need to be visited.

The Two-Watched-Literals algorithm tries to minimize the time by visiting only the necessary clauses. The premise of this algorithm is, that a clause can't start a unit propagation, as long as it contains at least two non false literals. This is based on the fact, that a clause is not satisfied, if all of its literals are false. So a unit propagation starts, if there is only one non false literal left. \cite{moskewicz2001chaff}

For the algorithm to work, the SAT solver needs to watch two non false literals in every clause. If a variable gets an assignment, then only clauses, whose watched literal turned false, need to be visited. In every visited clause the algorithm tries to swap out the false literal, with a non false replacement. If there is no other literal, that can act as a replacement, then the second watched literal must be propagated as a unit literal. This algorithm is efficient, because it only performs the bare minimum of clause visits, that are needed to ensure propagation completeness. \cite{moskewicz2001chaff}

\section{Variable State Independent Decaying Sum (VSIDS)}
\label{sec:VSIDS}

The decision heuristic VSIDS for choosing the next variable to assign, was first introduced in the paper "Chaff: Engineering an Efficient SAT Solver" \cite{moskewicz2001chaff} by Moskewicz et al.. The heuristic is based on the appearance of variables in a new clause, that is learned because of a conflict. Its direct application is therefore only possible in a CDCL based algorithm. In order to apply the heuristic, the SAT solver needs to keep a score for each variable, which is initialized with zero. After every conflict, the score of every variable, contained in the newly learned clause, needs to be incremented by one. For every new variable assignment, the SAT solver needs to select the variable, which currently has the highest VSIDS score and is unassigned. In order to prevent overflows, the score of each variable needs to be repeatedly divided by a constant integer. This heuristic has proven to be outperforming other heuristics, while still being very efficient to implement. The efficiency stems from the fact, that a score adjustment only happens after a conflict occurs, which in itself is a rare occurrence.

\subsection{Normalized Variable State Independent Decaying Sum (NVSIDS)}
\label{sec:NVSIDS}

In the paper "Adaptive Restart Strategies for Conflict Driven SAT Solvers" \cite{biere2008adaptive} the author Armin Biere describes a different implementation of the VSIDS heuristic, which is called "normalized VSIDS". In this implementation the score of each variable is punished after every conflict, by multiplying it with a factor between 0 and 1. Then the counter of each variable, that is involved in the conflict, gets incremented by 1 minus the punishment factor. The author describes, that this implementation leaves every variable with a VSIDS score between 0 and 1. The author mentions, that this method is very costly, because the score of every variable needs to be punished at every conflict. In contrast to that, the original VSIDS implementation halves the score of every variable after a certain amount of conflicts occurred, which is much less computationally expensive.

\subsection{Exponential Variable State Independent Decaying Sum (EVSIDS)}

A different implementation of the VSIDS heuristic is the exponential VSIDS, which was proposed in the paper "An extensible SAT Solver" \cite{een2003extensible} by E{\'e}n and S{\"o}rensson. 

In contrast to the NVSIDS implementation, EVSIDS only updates the score of every variable involved in the conflict. In order to update the score, EVSIDS uses a value \textbf{g} which must be larger than 1, and the index \textbf{i} of a conflict. The value $g^{i}$ then gets added to the score of every variable, that is involved in the conflict. This is much less computationally expensive then NVSIDS, because not every variable needs to be updated at every conflict. \cite{biere2008adaptive}

The author Armin Biere has also shown that the equations of NVSIDS and EVSIDS are linearly related, which is why they produce the same variable orderings \cite{biere2008adaptive}. So EVSIDS produces the same results as NVSIDS while still being less computationally expensive.

\section{The Literals Blocks Distance (LBD)}

The concept of the Literals Blocks Distance (LBD) was first introduced in the paper "Predicting learnt clauses quality" \cite{audemard2009predicting} by Audemard and Simon and is a metric to measure the quality of a newly learned clauses in CDCL based SAT solvers. The LBD score of a clause is defined as the number of distinct variable decision levels, that are contained in the clause. The authors explain, that every clause with an LBD score of two must also be a unique implication point, which makes them very good for learning in the CDCL algorithm. The authors use the LBD score, in order to determine which clauses to delete. So the deletion process starts when 20000 conflicts are reached, which prompts the solver to delete half of the learned clauses. These clauses are sorted by their LBD score. The number of conflicts, that trigger the deletion process, is then increased by 500 for every database reduction, that the solver performed.

Consider the following example of a clause:

\begin{leftbar}
\begin{center}
\begin{displaymath}
x_1 \vee x_2 \vee x_3 \vee x_4
\end{displaymath}
Decision levels: $x_1=1$, $x_2,x_3=2$, $x4=3$ 
\end{center}
\end{leftbar}

The clause in the example has exactly three distinct decision levels. The LBD score of the clause is therefore three.

\section{Restarts}

Restarts are a technique to prevent SAT solvers from spending a large amount of time in search space areas, that yield no solution \cite{biere2009handbook}. When the solver initiates a restart, it removes all variable assignments and starts from the beginning. If the SAT solver is deterministic in its strategy, then this would just slow the SAT solver down, because it would take the exact same path again. So in order for restarts to have an effect, the SAT solver either has to have a randomized component, or it needs to retain additional information from previous assignments. If the branching heuristic is randomized, then the SAT solver would take a different path after a restart, which might lead to a faster solution. The other possibility is especially relevant for the CDCL-algorithm. During conflicts in the CDCL-algorithm, the SAT solver learns a new clause, in order to resolve a conflict. If the solver retains these learned clauses after restarts, then these clauses will prevent the conflicts, that occurred before and lead the solver to a different path. This way the solver can have only deterministic components, while still being able to profit from restarts. \cite{biere2009handbook}

\section{Luby Restarts}

The Luby sequence was first introduced in the paper "Optimal Speedup of Las Vegas Algorithms" \cite{luby1993optimal} by the authors Luby et al. and is defined as follows:

\begin{definition}
\centering
\begin{leftbar}
$t_i =
\begin{cases}
  2^{k-1} & \text{if } i = 2^k - 1   \\
  t_{i-2^{k-1}+1} & \text{if } 2^{k-1} \leq i < 2^k - 1
\end{cases}$
\end{leftbar}
\caption{Luby sequence \cite{luby1993optimal}}
\end{definition}

It was first used in the TiniSat SAT solver, that was introduced in the paper "The Effect of Restarts on the Efficiency of Clause Learning" \cite{huang2007effect} by Jinbo Huang, where the author showed, that it has significant performance advantages compared to other restart policies.

\chapter{Related Work}
\label{ch:Related Work}

This chapter gives an overview of the different areas of SAT solving research, the current state of the art and how the approach of this thesis differs from the current research topics.

\section{Evolution of SAT Solvers}
First we take a look at historically relevant SAT solvers and the concepts they introduced, which are still used by many modern SAT Solvers today.

In the paper "Chaff: Engineering an Efficient SAT solver" \cite{moskewicz2001chaff} the authors Moskewicz et al. describe the implementation of their SAT solver "CHAFF". The two big contributions this SAT solver made, are the "Two-Watched-Literals" algorithm and the "Variable State Independent Decaying Sum (VSIDS) heuristic", which are still used today in many SAT solvers. The Two-Watched-Literals algorithm, as already described in \ref{sec:twoWatchedLiterals}, allows the SAT solver to only visit a clause, if it is absolutely necessary and therefore offers a drastic reduction of the computational complexity. The VSIDS heuristic, as described in \ref{sec:VSIDS}, is a branching heuristic, that assigns more weight to variables, that recently appeared in newly learned clauses. It is still used today in many modern SAT solvers, because it outperforms most other heuristics.

The paper "An Extensible SAT-solver" \cite{een2003extensible} by the authors E{\'e}n and Sörensson describes the implementation of the minimal SAT solver "MiniSat". The paper presents an efficient implementation of a SAT solver, that makes use of the CDCL-algorithm and is inspired by the SAT solver Chaff \cite{moskewicz2001chaff}. An important contribution of the MiniSat SAT solver, is a different implementation of the VSIDS heuristic, which will later on be referred to as exponential VSIDS (EVSIDS) in literature. The original VSIDS heuristic increments the score of every variable, that is involved in the conflict and then decays the score of every variable periodically. In contrast to that, EVSIDS has no inbuilt score decay, but instead increases the score of each conflict variable by exponentially growing values. In order to prevent overflow, the scores eventually have to be adjusted.

In the paper "Adapative Restart Strategies for Conflict Driven SAT Solver" \cite{biere2008adaptive} Armin Biere introduces another implementation of VSIDS called the normalized VSIDS (NVSIDS) heuristic. As already explained in \ref{sec:NVSIDS}, the advantage that NVSIDS offers, are scores between 0 and 1. Acording to the author these scores then reflect the percentage of how often a variable was part of a conflict. The disadvantage of NVSIDS is, that the score of every variable needs to be adjusted after every conflict. Armin Biere then proves, that EVSIDS produces the same variable order as NVSIDS. EVSIDS therefore has the same advantages as NVSIDS, while being more efficient, because only the scores of variables involved in a conflict need an adjustment.

In the paper "Predicting learnt clauses quality in modern SAT solvers" \cite{audemard2009predicting} the authors Audemard and Simon describe their SAT solver "GLUCOSE", which also makes use of conflict driven clause learning. With their solver they try to solve the problem of CDCL-solvers either using up too much memory, because of the learned constraints, or forgetting too many clauses and therefore making the solving process slower. The core of the authors work is the identification of good clauses to learn. They defined the "Literals Blocks Distance" (LBD), which is defined as the number of subsets of literals in a clause, that belong to the same decision level. This score is then computed for every new learned clause and every clause can alternatively be re-scored, when it is used in the unit propagation procedure. The authors make use of the LBD score during the deletion process of learned clauses. They use a strategy that deletes half of all learned clauses every 20000 + 500 * x conflicts, with x referring to the number of conflicts that already occured. The LBD score is used to determine, which of the learned clauses get deleted. The clauses with a lower LBD score get priority over the ones with high LBD scores.

In conclusion we can see, that all of the SAT solvers mentioned above, offered new techniques in the areas of unit propagation, branching heuristics and learned clause database reduction, many of which are still used today in modern SAT solvers. One characteristic, that all of these solvers share, is that they only work with clauses. In this thesis we will try to incorporate the techniques mentioned above, in a new SAT solver architecture, that uses different constraint types.

\section{SAT Solvers with Different Constraint Types}
This section focuses on related work in the area of SAT solvers, that support several constraint types.

The paper "To Encode or to Propagate? The Best Choice for Each Constraint in SAT" \cite{abio2013encode} by Abio et al. examines, in which situations it is better to use a CNF encoding of constraints, instead of implementing the native support directly in the SAT solver. They specifically talk about the work in the paper "Solving SAT and SAT Modulo Theories: From an Abstract
Davis–Putnam–Logemann–Loveland Procedure to DPLL(T)" \cite{nieuwenhuis2006solving} by Nieuwenhuis et al., which features a solver, that natively supports cardinality constraints. It also has the ability to resolve cardinality constraint conflicts by learning clauses. It can also adapt and change to strict CNF encoding, if certain conditions are met. The results show, that this adaptive strategy, which changes between native constraints and encodings, can have advantages in certain benchmarks. It can also be a hindrance because of the large amount of clauses, that it can generate, in order to resolve conflicts.

Another SAT solver, that is capable of natively solving cardinality constraints and pseudo-boolean constraints, was presented in the paper "The Sat4j Library 2.2, System Description" \cite{le2010sat4j} by Le Berre and Parrain. Sat4j contains two modules, that each are capable of natively solving cardinality constraints. The module Sat4j PB Res uses simple conflict resolution by treating each of the constraints as simple clauses. This makes the resolution process easier. The module Sat4j PB CP uses the cutting planes method, which is more complex and therefore more computationally expensive. In most benchmarks the resolution based module performs better, but the cutting planes method is more effective in specific problems like the pigeon hole problem.

In the paper "Extending SAT Solvers to Cryptographic Problems" \cite{DBLP:conf/sat/SoosNC09} the authors Soos et al. describe the implementation of their SAT solver CryptoMiniSat, which natively supports XOR constraints, in order to efficiently solve cryptographic problems. This is necessary, because the encoding of an XOR constraint as a CNF can potentially result in an exponential amount of clauses. The native support is achieved by representing the XOR constraint as a normal clause, that changes its appearance based on different variable assignments. This native support results in faster solving speed for cryptographic problems.

In conclusion there are several different solvers, that already support other constraint types like AMO constraints, cardinality constraints or XOR constraints natively. To the best of our knowledge there currently is no research on a SAT solver, that natively supports DNF constraints.

\chapter{Analysis}
\label{ch:Analysis}

This chapter focuses on the new kinds of constraints, that the novel SAT solver is supposed to be able to solve in addition to the normal CNF input. In order for the SAT solver to process these new constraints, a thorough analysis on how these constraints interact with each other during a conflict and what the solver can learn from that, is needed.

\section{Basic SAT Solver Structure}

This thesis is concerned with building a novel SAT solver architecture, that is capable of solving basic CNF formulas like other modern SAT solvers, while also being capable of handling DNF and AMO constraints. The first important step is therefore the decision, what techniques our architecture is going to use, in order to solve basic CNF formulas.

The DPLL algorithm, as already described in section \ref{sec:dpll}, is one of the most prominent algorithms for SAT solving. It is a depth-first search based algorithm and was developed in order to combat the high space requirements of the DP algorithm \cite{biere2009handbook}. Because the DPLL algorithm uses a search based approach, the time that is needed to solve a formula, can be really long, depending on how large the formula is. This is the case, because the DPLL algorithm just tries every possible combination of variable assignments, until a satisfying assignment is found. There are techniques, that allow for a faster search like unit propagation, phase saving, branching heuristics and also restarts \cite{biere2009handbook}, but many modern SAT solvers still don't use a pure DPLL-based approach.

Instead the current best SAT solvers like Kissat \cite{BiereFazekasFleuryHeisinger-SAT-Competition-2020-solvers} and CaDiCaL \cite{Biere-SAT-Competition-2017-solvers} instead make use of the CDCL algorithm. As already explained in section \ref{sec:cdcl}, this allows for non-chronological backtracking and the learned clauses make sure, that non satisfiable areas of the search space are never reached. This can reduce the solving time of Boolean formulas drastically. 

For this thesis the integration of different constraint types into these modern algorithms is the most important aspect of this work. The usage of the DPLL algorithm with different constraint types would only allow us to explore optimization options in the area of unit propagations, branching heuristics and restarts, because there is no real conflict resolution. By using the CDCL algorithm it is possible to explore how the constraints interact directly with each other via conflicts, and how this can be leveraged by learning new constraints. Both approaches are interesting and because it is difficult to assess, which of these algorithms offers the best performance, when the formula contains different constraint types, we decided to implement both algorithms and evaluate their performance.

\subsection{Branching Heuristics}

The branching heuristic is an important part of every SAT solver. A smart decision of which variable to assign next, can have a significant impact on the solving time of a formula. In the paper "The Impact of Branching Heuristics in
Propositional Satisfiability Algorithms" \cite{marques1999impact} Marques-Silva has shown, that the number of decisions a SAT solver has to make, can vary strongly depending on the branching heuristic. One of the results suggests, that a randomized branching heuristic often yields good results. Because branching heuristics have such a significant impact on the solving speeds, we can't ignore this aspect and have to analyze which heuristic would be optimal for our solver. In 2001 the authors Moskewicz et al. \cite{moskewicz2001chaff} then introduced their new branching heuristic "VSIDS", which is used in their SAT solver "Chaff". The authors report, that the VSIDS branching heuristics yielded better performance on hard benchmarks, while not negatively impacting performance on any of the easier benchmarks \cite{moskewicz2001chaff} compared to other branching heuristics. With this significant improvement, the VSIDS heuristic became an important part of modern SAT solvers, that make use of the CDCL-algorithm \cite{biere2015evaluating}. 

A new variant of this heuristic, which is now referred to as "exponential VSIDS" (EVSIDS) \cite{biere2015evaluating} was introduced in the paper "An extensible SAT Solver" \cite{een2003extensible}. 

EVSIDS is efficient, because it only updates the score of each bumped variable. It can then be used in combination with a priority queue, which keeps the variables in order of their current scores. The variables are then polled from the queue, until an unassigned variable is found. During backtracking the unassigned variables need to then be inserted back in to the queue. \cite{biere2015evaluating}

Due to the large performance improvement that the VSIDS heuristic brings to the SAT solving process and the efficient implementation,  that is offered by EVSIDS, we decided to implement this variant of the heuristic.

\subsection{Constraint Database Reduction}

The clause database reduction is a concept, that is needed in every SAT solver, that makes use of the CDCL-algorithm. During the learning process, the database of constraints grows due to the new constraints, that are learned at every conflict. At a certain point the benefit of information, that these constraints provide, is overshadowed by the memory usage and computational complexity while propagating in all these constraints. The SAT solver therefore needs a mechanism to reduce the database size. \cite{biere2009handbook}

Of course we want to retain the most important learned information, which is why it is important, to retain the most useful learned constraints. In the paper "Predicting Learnt Clauses Quality in Modern SAT Solvers" \cite{audemard2009predicting} the authors Audemard and Simon describe their measurement of clause quality by using the LBD score. By using this clause deletion scheme based on the LBD score, their SAT solver managed to gain a significant performance advantage.

The problem with implementing this type of clause deletion scheme in our novel SAT solver are the different types of constraints, that it uses. The LBD score was specifically defined to measure the quality of clauses, but our SAT solver uses AMO and DNF constraints in addition to clauses. So in order to make us of this scheme we first have to define the LBD score for general constraints

\begin{definition}[!htb]
\begin{leftbar}
Given a partial assignment of variables, the LBD score of a constraint is the number of distinct decision levels \textbf{n} that can be counted in the constraint.
\end{leftbar}
\caption{Definition of the LBD score}
\label{def:LBD}
\end{definition}

The Definition \ref{def:LBD} is a direct translation of the clause LBD score to a general constraint. It is hard to predict, if these general LBD scores also have a significant performance advantage over other database reduction schemes.

\subsection{Restarts}

Another important aspect of modern SAT solvers are the restarts. Restarts in SAT solving reset the current state of the solving process and then restart it. In order for those restarts to have an advantage, they either have to be randomized, so the solver won't just try the same combinations after the restarts, which would yield no advantage. Or the SAT solver needs to retain information like learned constraints, which guide the solving process into a new direction after a restart.
\cite{biere2009handbook}

In the paper "Evaluating CDCL Restart Schemes" \cite{biere2015evaluatingRestarts} the authors Biere and Fröhlich evaluate several static restart schemes with uniform and non-uniform restart intervals, as well as more modern dynamic restart schemes. The results show, that the dynamic restart schemes like the "Glucose restart scheme" \cite{biere2015evaluatingRestarts} outperform the other static schemes. One important aspect of this dynamic scheme is the use of the LBD score of recently learned clauses. The LBD score is a concept, that was defined for clauses, which makes it harder to predict how this scheme will perform in our novel SAT solver architecture, which makes use of several different constraints. We therefore decided to use a static scheme, that isn't reliant on specific characteristics of the learned constraints.

The authors also compare a static restart scheme with uniform restart intervals with non-uniform schemes like the Luby-series and the inner/outer strategy. The inner/outer strategy performed worse than both, and while the Luby-series and the uniform restart scheme showed similar performance in general, the Luby-series was better in a broader range of benchmarks. \cite{biere2015evaluatingRestarts}

Because of our concerns with directly translating the LBD score for restarts, we decided to implement a version of the Luby-series for the restarts in our novel SAT solver architecture.

\section{"At Most One" (AMO) Constraint}
\label{sec:AMOConstraint}

This section describes the approach on how the novel SAT solver can deal with AMO constraints and their conflicts. The AMO constraint is a Boolean formula, that can only be true, if either zero or one of the literals is true.  Formally this constraint can be defined in its CNF – form as:

\begin{definition}
\begin{leftbar}
\begin{displaymath}
AMO(x_1, ..., x_n)= \{\neg x_i \vee \neg x_j \; | \; \forall i, j \; i < j\}
\end{displaymath}
\end{leftbar}
\caption{Naïve CNF encoding of an AMO constraint \cite{biere2009handbook}}
\label{def:AMOEncodingNaive}
\end{definition}

The Definition \ref{def:AMOEncodingNaive} shows the naïve CNF encoding of an AMO constraint. 

It uses n * (n-1) / 2 clauses and can therefore grow very large. There are also other encodings like the sequential AMO encoding, which uses 3n – 4 clauses and n – 1 auxiliary variables. Another encoding with fewer auxiliary variables is the logarithmic bitwise encoding which requires only n log n clauses and n auxiliary variables. 
\cite{chen2010new} 

All of these encodings have in common, that they try to represent the AMO constraint in form of clauses. This is necessary for most of the common SAT solvers, because they can only solve formulas in CNF form. Our novel SAT solver architecture shouldn’t be restricted to clauses. Our architecture is supposed to natively support AMO constraints and therefore internally differentiate between different constraint types. Therefore we can use a simple encoding with the following form:

\begin{leftbar}
\begin{displaymath}
AMO \; x_1 ... x_n
\end{displaymath}
\end{leftbar}
By using this type of encoding, only a single constraint input is needed, in order to represent the AMO constraint. If a conflict arises, our solver can then use different strategies, depending on which types of constraints are involved in the conflict.

\subsection{AMO Constraint Unit Propagation}

In order to do a conflict analysis, it is important to understand how the propagation of literals work in a natively supported AMO constraint. By using the simple one line encoding explained above, the solver can remember which literals belong to an AMO constraint. Consider the situation, that a variable gets assigned a value during the solving process. If in consequence to this assignment a literal turns false in any AMO constraint, then the solver doesn’t have to visit them. This is possible, because the AMO constraint stays satisfied, even when all of the literals turn false. Therefore the solver doesn't need to consider the literals, that turn false in an AMO constraint. It is only important to consider the literals, that turn true in any AMO constraint. If a literal in an AMO constraint turns true, then every other literal in that AMO constraint automatically has to turn false, in order to satisfy the constraint.

\subsection{AMO - AMO Conflicts}

This section contains the analysis of the different conflict types, that can occur between two AMO constraints, and an analysis on how these conflicts can be resolved.


First it is important to understand, how a conflict can occur between two AMO constraints. Consider two general AMO constraints $AMO_1$ and $AMO_2$. A non satisfying assignment in an AMO constraint occurs, when at least two literals in the constraint turn true. So in order for two AMO constraints to have a conflict, a literal assignment that occurs in $AMO_1$, needs to be responsible for at least two true literals in $AMO_2$. This situation can only happen, if the the AMO constraints are complementary on at least one literal. This means that one AMO constraint contains a literal $x_k$ while the other AMO constraint has a literal $\neg x_k$. This is necessary, because an AMO constraint can only force other literals to be false. So when one of the constraints forces a literal to be false, the other AMO constraint needs to have the complementary literal, in order for it to turn true and cause a conflict.

\subsubsection{First AMO - AMO Conflict}

In order to understand the first type of AMO-AMO conflict, consider the following two constraints $AMO_1$ and $AMO_2$:

\begin{example}
\begin{leftbar}
\begin{displaymath}
AMO_1(a,b,c,d)
\end{displaymath}
\begin{displaymath}
AMO_2(e,g,\neg c,f)
\end{displaymath}
\begin{displaymath}
d = 1, f = 1
\end{displaymath}
\end{leftbar}
\caption{Example for the first type of AMO-AMO conflict}
\label{ex:FirstAMOAMO}
\end{example}

The Example \ref{ex:FirstAMOAMO} shows two AMO constraints that currently have a conflict. Because the literals $\{d,f\}$ are currently true, it forces a unit propagation in both constraints. Because of these unit propagations the literals $c$ and $\neg c$ both have to be true at the same time, which causes a conflict.

This type of conflict occurs between AMO constraints, that are complementary on a single literal. It can be resolved by using the rule of the following lemma:

\begin{lemma}
\begin{leftbar}
Consider two AMO constraints $AMO_1$ and $AMO_2$. Both constraints are complementary on the variable $x_a$. $AMO_1$ contains exactly one literal $x_a$ with $AMO_2$ containing the complementary literal $\neg x_a$. Let $\{y_1,...,y_k\}$ be the set of literals of $AMO_1$ without $x_a$ and $\{z_1,...,z_m\}$ be the set of literals of $AMO_2$ without $\neg x_a$. 
If a conflict occurs between these constraints, then it can be resolved by learning the following clauses:
\begin{displaymath}
\{\neg y_i \vee \neg z_j | y_i \in \{y_1,...,y_k\}, z_j \in \{z_1,...,z_m\}, \{y_i,z_j\} \not\in \{x_a, \neg x_a\}\}
\end{displaymath}
\end{leftbar}
\label{le:oneComplementary}
\end{lemma}

\begin{proof}
We first have to establish, that every variable will always have an assignment at the end of the solving process. This also means, that the variable $x_a$ will have an assignment. Because the AMO constraints are complementary on this variable, this means that at least one of them will have a true literal. This forces a unit propagation of all other literals in this AMO constraints. So therefore either $\{y_1,...,y_k\}$ or $\{z_1,...,z_m\}$ have to all be false, which can be written as $(\neg y_1 \wedge ... \wedge \neg y_k) \vee (\neg z_1 \wedge ... \wedge \neg z_m)$. If any literal other than $x_a$ or $\neg x_a$ turns true, then this constraint forces all other literals of one of the AMO constraints to be false. This AMO constraint then can't have two true literals anymore. This resolves the conflict. This expression can then be transformed into the clauses from the rule above.
\end{proof}

Now by applying the Lemma \ref{le:oneComplementary} to the Example \ref{ex:FirstAMOAMO}, the conflict can be resolved the following way:
\begin{displaymath}
AMO_1(a,b,c,d)
\end{displaymath}
\begin{displaymath}
AMO_2(e,g,\neg c,f)
\end{displaymath}
In this case either $c$ or $\neg c$ has to be true. Then the following constraint needs to be satisfied:
\begin{displaymath}
(\neg a \wedge \neg b \wedge \neg d) \vee (\neg e \wedge \neg g \wedge \neg f)
\end{displaymath}
This constraint can then be transformed to the following clauses:
\begin{displaymath}
(\neg a \vee \neg e) \wedge (\neg a \vee \neg g) \wedge (\neg a \vee \neg f) \; \wedge
\end{displaymath}
\begin{displaymath}
(\neg b \vee \neg e) \wedge (\neg b \vee \neg g) \wedge (\neg b \vee \neg f) \; \wedge
\end{displaymath}
\begin{displaymath}
(\neg d \vee \neg e) \wedge (\neg d \vee \neg g) \wedge (\neg d \vee \neg f)
\end{displaymath}

\subsubsection{Alternative Way to Apply the Rule}

The general rule to resolve conflicts between AMO constraints, that are complementary on a single literal, results in a large amount of clauses. The rule is defined to completely remove every possible conflict between the two AMO constraints, but it is not always necessary to learn all of the clauses, in order to avoid future conflicts..

\begin{lemma}
\begin{leftbar}
Consider two AMO constraints $AMO_1$ and $AMO_2$. Both constraints are complementary on the variable $x_a$. $AMO_1$ contains exactly one literal $x_a$ with $AMO_2$ containing the complementary literal $\neg x_a$. Let $y_i$ be a literal that is true in $AMO_1$ and not $x_a$. Let $z_j$ be a literal that is true in $AMO_2$ and not $\neg x_a$. Then this specific conflict can be avoided by learning the following clause:
\begin{displaymath}
\neg y_i \vee \neg z_j
\end{displaymath}
\end{leftbar}
\end{lemma}

\begin{proof}
Because there is a conflict between the two AMO constraints, one of the AMO constraints needs to have two true literals. Without loss of generality let this be $AMO_2$ with the literals $\neg x_a$ and $z_j$ being true. Because the constraints are conflicting $AMO_1$ then needs to have one literal $y_i$, which resulted in $x_a$ being  false and therefore $\neg x_a$ being true. Now in order to resolve this specific conflict, either $z_j$ needs to be false, so that $AMO_2$ is satisfied, or $y_i$ needs to be false, so that $\neg x_a$ won't turn true. This can be expressed as $\neg y_i \vee \neg z_j$.
\end{proof}

This rule doesn't completely avoid every conflict between the two constraints, because there could be another pair of true literals $\{y_x,z_y\}$ which causes a conflict later on, but this rule is more efficient, because the solver only has to learn a single clause.

\subsubsection{Second AMO - AMO Conflict}

This section describes the conflict resolution rules for AMO constraints, that are complementary on exactly two literals. First take a look at the following example:
\begin{example}[H]
\begin{leftbar}
\begin{displaymath}
AMO_1(a,b,c,d)
\end{displaymath}
\begin{displaymath}
AMO_2(e,f,\neg c,\neg d, \neg g)
\end{displaymath}
\begin{displaymath}
b = 1
\end{displaymath}
\end{leftbar}
\caption{Example of a conflict between AMO constraints with two complementary literals}
\label{ex:secondAMOAMO}
\end{example}
The Example \ref{ex:secondAMOAMO} shows a conflict between AMO constraints with two complementary literals. Because the literal $b$ is true, $AMO_1$ forces $\{a,c,d\}$ to be false. Now $AMO_2$ has the literals $\{\neg c,\neg d\}$, that are true and is therefore not satisfied. To resolve this conflict, the following lemma can be used:

\begin{lemma}
\begin{leftbar}
Consider two AMO constraints $AMO_1$ and $AMO_2$. $AMO_1$ contains exactly two literals $x_a$ and $x_b$ with $AMO_2$ containing the complementary literals $\neg x_a$ and $\neg x_b$.
If a conflict occurs between these constraints, then it can be resolved by learning the following clauses:
\begin{displaymath}
\{\neg x \; | \; x \in \{AMO_1,AMO_2\}, x \notin \{x_a,x_b,\neg x_a, \neg x_b\}\}
\end{displaymath}
\end{leftbar}
\label{le:twoComplementary}
\end{lemma}

\begin{proof}
In order to proof this lemma, consider the situation, that any literal other than \newline $\{x_a, x_b, \neg x_a, \neg x_b\}$ turns true. Because of the unit propagation process of AMO constraints, either $\{x_a, x_b\}$ or $\{\neg x_a, \neg x_b\}$ would turn false in this situation. This would result in their complementary literals being true. Therefore one of the AMO constraints would contain at least two true literals, which results in them not being satisfied. So what can be concluded from that, is that all literals except for the set $\{x_a, x_b, \neg x_a, \neg x_b\}$ need to be false. This can be achieved by learning the unit clauses $\{\neg x \; | \; x \in \{AMO_1,AMO_2\}, x \notin \{x_a,x_b, \neg x_a, \neg x_b\}\}$.
\end{proof}

By using Lemma \ref{le:twoComplementary} on the Example \ref{ex:secondAMOAMO}, the conflict can be resolved by learning the unit literals $\{\neg a, \neg b, \neg e, \neg f, g\}$.

\newpage
\subsubsection{Third AMO - AMO Conflict}

This section describes the conflict resolution between AMO constraints with three or more complementary literals. First consider the following example:

\begin{example}
\begin{leftbar}
\begin{displaymath}
AMO_1(a,b,c,d)
\end{displaymath}
\begin{displaymath}
AMO_2(\neg a, \neg b, \neg c, e)
\end{displaymath}
\end{leftbar}
\caption{Conflict between AMO constraints with three complementary literals}
\label{ex:threeComplementary}
\end{example}

The Example \ref{ex:threeComplementary} shows two AMO constraints, that are complementary on three literals. There is no possible assignment, that satisfies both constraints.

\begin{lemma}
\begin{leftbar}
Consider the AMO constraints $AMO_1$ and $AMO_2$. If the AMO constraints are complementary on at least three literals, then the formula is not satisfiable.
\end{leftbar}
\end{lemma}

\begin{proof}
If there are at least three literals in $AMO_1$, that are also in their negated form in $AMO_2$, then there is no possible assignment, that satisfies both constraints. As already proven by Lemma \ref{le:twoComplementary}, if there are two literals $x_a$ and $x_b$ in $AMO_1$, that are also in their negated form in $AMO_2$, then all other literals in $AMO_1$ and $AMO_2$ have to be false. Now consider the situation where there is an extra literal $x_c$ in $AMO_1$ and $\neg x_c$ in $AMO_2$. Because either $x_a$ or $x_b$ are already true, $x_c$ needs to be false. This in turn causes $\neg x_c$ to be true. $AMO_2$ now has two true literals and can't be satisfied. Therefore the whole formula can be declared unsatisfiable.
\end{proof}

\subsection{AMO - Clause Conflict}

This section describes the conflict resolution between AMO constraints and clauses. First consider the following example:

\begin{example}
\begin{leftbar}
\begin{displaymath}
a \vee b \vee c
\end{displaymath}
\begin{displaymath}
AMO(e,f,c)
\end{displaymath}
\begin{displaymath}
a = 0, b = 0, e = 1
\end{displaymath}
\end{leftbar}
\caption{Conflict between an AMO constraint and a clause}
\label{ex:amoClauseConflict}
\end{example}

The Example \ref{ex:amoClauseConflict} shows a conflict between an AMO constraint and a clause. In this example the literals $\{a,b\}$ are already false, which forces a unit propagation of the literal $c$. Because the literal $e$ is already true, this causes a conflict with the AMO constraint. To resolve this conflict, the following lemma can be used:

\begin{lemma}
\begin{leftbar}
Consider a clause and an AMO constraint. Let $x$ be the literal, that causes a conflict between the two constraints. Let $\{y_1,...,y_m\}$ be the rest of the literals in the clause and $\{z_1,...,z_k\}$ the rest of the literals in the AMO constraint.
This conflict can then be resolved by learning the following constraint
\begin{displaymath}
\bigwedge_{a=1}^{k}((\bigvee_{i=1}^{n}y_i)\vee \neg z_a)
\end{displaymath}

\end{leftbar}
\label{le:amoClauseConflict}
\end{lemma}

\begin{proof}
In order for a conflict between a clause and an AMO constraint to occur, there must be a literal $x$, that is shared by both. A conflict happens, when $x$ needs to be true, because the set of literals $\{y_1,...,y_m\}$ is currently false and at the same time there exists a literal $z_i$ in the AMO constraint, that is currently true, which forces $x$ to be false. One possibility to resolve this conflict, is that one of the literals in $\{y_1,...,y_m\}$ turns true, which means that $x$ doesn't have to be true anymore. The other possibility is, that all the literals in $\{z_1,...,z_k\}$ turn false, so that it is possible for $x$ to turn true. This can be expressed as $\bigvee_{i=1}^{n}y_i \vee (\bigwedge_{a=1}^{k}\neg z_a)$. If this rule is then transformed into clauses, the solver learns $\bigwedge_{a=1}^{k}((\bigvee_{i=1}^{n}y_i)\vee \neg z_a)$.
\end{proof}

By applying the Lemma \ref{le:amoClauseConflict} to the Example \ref{ex:amoClauseConflict} the solver learns the following clauses:

\begin{leftbar}
\begin{displaymath}
a \vee b \vee \neg e
\end{displaymath}
\begin{displaymath}
a \vee b \vee \neg f
\end{displaymath}
\end{leftbar}

\subsubsection{Alternative AMO-Clause Conflict Resolution}
This section shows an alternative AMO-Clause Conflict resolution rule.

\begin{lemma}
\begin{leftbar}
Consider a clause and an AMO constraint. Let $x$ be the literal, that causes a conflict between the two constraints. Let $\{y_1,...,y_m\}$ be the rest of the literals in the clause and $z$ the literal in the AMO constraint, that is currently true. This specific conflict can be resolved by learning the following clause:
\begin{displaymath}
y_1 \vee ... \vee y_m \vee \neg z
\end{displaymath}
\end{leftbar}
\end{lemma}

\begin{proof}
Because there is a conflict between the AMO constraint and the clause, there must be a literal $x$, that is shared by both constraints. As already explained before, the literals $\{y_1,...,y_m\}$ must be false, which forces $x$ to be true. And there must be a literal $z$, that is true in the AMO constraint, which forces $x$ to be false. Otherwise there can't be a conflict. In order to resolve this specific conflict, it is enough to learn the constraint $y_1 \vee ... \vee y_m \vee \neg z$. Now either $z$ will be false, which allows $x$ to be true. Or one of the literals $\{y_1,...,y_m\}$ will be true, which allows $x$ to be false.
\end{proof}

With this rule, the solver only prevents this specific conflict. This is more efficient, because the solver only has to learn a single clause, but it doesn't completely prevent every conflict between the AMO constraint and the clause.

\section{DNF Constraint}
The third constraint type is the "disjunctive normal form" (DNF) constraint.
\begin{leftbar}
\begin{displaymath}
DNF(x_1,...,x_n) = \{x_1 \vee ... \vee x_n \}
\end{displaymath}
\begin{displaymath}
x_i = y_1 \wedge ... \wedge y_m
\end{displaymath}
\end{leftbar}
The DNF is a disjunction of conjunctions. It is satisfied, if at least a single conjunction is satisfied. This is the case, if every literal in that conjunction is true. In this thesis these conjunctions are called terms.

\subsection{Unit Propagation}
A unit propagation in a DNF can occur in one of two cases:
\begin{leftbar}
\begin{enumerate}
\item If there is only one non false term left, then all of its literals are unit literals
\item If all non false terms share a literal, then this literal is a unit literal.
\end{enumerate}
\end{leftbar}
The first case is trivial, because at least one term needs to be satisfied, in order for the DNF to be satisfied. Because there is only a single non false term left, that can fulfill this condition, every literal of that term needs to be true.
\par
For the second case consider a DNF with at least two terms, that are currently non false. The rest of the terms are false. If those terms share a literal $x$ then both of them can only be true, if and only if $x$ is true. Because at least one of them needs to be true, $x$ must be a unit literal.
\par
Now consider a situation where those constraints don't share any literal. In this situation there are several possibilities on how the DNF can be satisfied and therefore there can't  be a unit propagation yet.

\subsection{Efficient Literal Propagation for DNF Constraints}

The literal propagation process for DNF constraints is a difficult task, because the unit propagation starts, when all non false terms are intersecting on at least one literal. So during the literal propagation process, the solver needs to keep track of these intersections, otherwise it could miss a unit propagation. 

Generally the propagation needs to be efficient, because this process gets repeated many times during the solving procedure. The two-watched-literal algorithm for clauses achieves this, by minimizing the amount of time, where the solver visits clauses. So the first idea is to create an algorithm, that achieves a similar property for DNF constraints. Because clauses are just a special case of DNF constraints, one could consider watching only two non false terms at a time. While this would minimize the time, that is spent by visiting the DNF constraints, it is possible to create examples, where this algorithm misses necessary unit propagations.

\begin{example}[!htb]
\begin{leftbar}
Consider the following constraints:
\begin{displaymath}
DNF ((a,b),(a,c),(b,c,x))
\end{displaymath}
\begin{displaymath}
DNF ((a,b,c),(a,c,d),(a,b,d),(b,c,d,x))
\end{displaymath}
\end{leftbar}
\caption{Example where the Two-Watched-Term algorithm fails}
\label{ex:NotPropagationComplete}
\end{example}

The Example \ref{ex:NotPropagationComplete} shows a situation where such an algorithm misses a unit propagation. In the first DNF constraint there are three terms, that are all pairwise intersecting. If the algorithm only watches two of these terms, then it is possible to construct a partial assignment, where a unit propagation is missed. For this consider the situation, that $\{(a,b),(a,c)\}$ are watched. If $x$ turns false, then the DNF constraint isn't visited. The algorithm now misses the needed unit propagation of $a$. If $a$ now turns false, then the DNF constraint isn't satisfied. In this situation the algorithm would have to watch all three of the terms.
In a similar manner, it is possible to construct a partial assignment, that leads to a missed unit propagation in the second DNF constraint. In this example the algorithm would have to watch four terms, in order to not make a mistake. This can be extended for a variable amount of terms, so in order to only watch a subset of terms in every DNF constraint, the algorithm has to calculate how many terms it needs to watch to still be propagation complete. 

A simple solution to this problem, is to just watch all terms in every DNF constraint.
On every visit the algorithm can then just select the shortest non false term in the DNF constraints and check for intersections with all other non false terms. If there exists such a literal, then it needs to be propagated. In order to speed up the search for the shortest term, it is efficient to already sort all terms by their size. But this algorithm has the problem, that it results in many unnecessary visits to the DNF constraints. If the DNF constraints are very large then these visits take too much time.

\subsection{The Two-Watched-Termsets-Algorithm}

This section describes the Two-Watched-Termsets-Algorithm, which is a compromise between the two approaches explained above. The algorithm changes its behavior depending on how many non intersecting terms are left in the DNF constraint. In order to understand how this algorithm ensures propagation completeness, it is necessary to look at all possible cases.

Similar to unit clauses, which contain only a single literal, there are also DNF constraints, that contain unit literals. A unit literal in a DNF constraint, that is not the result of a partial variable assignment, is a literal, that is contained in every term.
Because such a unit literal gets propagated in the beginning, the algorithm doesn't consider them during the calculation of intersections and they are therefore also not considered in the cases below.


\paragraph{First case:} After the creation of the DNF constraint, the algorithm can't find any non-false non-intersecting terms. If this is the case, then the algorithm automatically watches every term of the DNF constraint.

\paragraph{Second case:} The DNF constraint contains two non-false non-intersecting terms. As already explained, a unit propagation in a DNF constraint, can only occur, if there is only one non-false term left, or if all non-false terms intersect on a literal. So as long as the algorithm watches two non-false non-intersecting terms, it can never miss a unit propagation. So in this case the algorithm has to only watch these two terms.

\paragraph{Propagation:}
So after the initial creation of a DNF constraint the algorithm either watches all the terms, or only two terms. If a propagation occurs in a DNF constraint, where all terms are watched, the algorithm can just check for literal intersections in all non-false terms and then propagate these intersections as unit literals.

If a propagation occurs in a DNF constraint, where only two terms are watched, it is more complicated. Because both watched terms don't intersect, only one of them can turn false as a result of a propagation. The algorithm then has to try to replace the false watched term. This replacement also can't intersect with the currently watched non-false term. If the algorithm can find a suitable replacement, then there is no need for a unit propagation. If it can't find a replacement, then it now needs to watch all terms, that intersect with the current non-false watched term. This also means, that it needs to propagate every shared literal of these terms as a unit literal. The behavior after this point is then similar to when all terms are watched.

A big adavantage of this algorithm is, that there is no backtracking needed, in order to ensure the propagation completeness. At every point the algorithm either watches two non-false non-intersecting terms, or all non-false terms, that are currently intersecting.


\newpage
\subsection{DNF - DNF Conflict}

This section describes how a conflict between two DNF constraints can be resolved. The notation $DNF/x_i$ means, that every term, that contains the literal $x_i$ gets removed from the DNF constraint. First consider the following example:

\begin{example}[!htb]
\begin{leftbar}
\begin{displaymath}
DNF_1 ((a,b),(c,d))
\end{displaymath}
\begin{displaymath}
DNF_2 ((e,\neg b),(f,g))
\end{displaymath}
\begin{displaymath}
c = 0, f = 0
\end{displaymath}
\end{leftbar}
\caption{Example of a conflict between two DNF constraints}
\label{ex:DNFDNFConflict}
\end{example}
The Example \ref{ex:DNFDNFConflict} shows a conflict between two DNF constraints. Because the literals $\{c,f\}$ are already false, the literals $b$ and $\neg b$ need to both be true, in order to satisfy both constraints, which causes a conflict.

In order to resolve this conflict, the following rule can be applied:

\begin{lemma}
\begin{leftbar}
Consider two DNF constraints $DNF_1$ and $DNF_2$, that are complementary on the variable $x$. $DNF_1$ contains the literal $x$ and $DNF_2$ contains the literal $\neg x$. A conflict, that is caused by the variable $x$, can then be resolved by learning the following constraints:
\begin{displaymath}
DNF_1 / x \vee DNF_2 / \neg x
\end{displaymath}
\end{leftbar}
\label{le:DNFDNFConflict}
\end{lemma}

\begin{proof}
The two DNF constraints are complementary on the variable $x$. If a term in $DNF_1$, that contains the literal $x$ is satisfied, then every term in $DNF_2$, that contains the literal $\neg x$ turns false. The same principle is true when the situation is reversed and a term in $DNF_2$ with the literal $\neg x$ is satisfied. That means, that in order for both constraints to be satisfied, either $DNF_1/x$ or $DNF_2/\neg x$ needs to be true. Therefore the conflict can be resolved by learning the constraints $DNF_1 / x \vee DNF_2 / \neg x$.
\end{proof}

By applying the Lemma \ref{le:DNFDNFConflict} to the Example \ref{ex:DNFDNFConflict}, the conflict can be resolved by learning the following constraint:
\begin{leftbar}
\begin{displaymath}
 DNF ((c,d),(f,g))
\end{displaymath}
\end{leftbar}

\newpage
\subsubsection{Alternative DNF-DNF Conflict Resolution}

When the solver uses the Lemma \ref{le:DNFDNFConflict}, it only has to learn a single DNF constraints, in order to resolve a DNF-DNF conflict. The next example shows, that even learning a single DNF constraint can be an unnecessary burden for the solver.

\begin{example}
\begin{leftbar}
\begin{displaymath}
DNF((a,b),(d,e))
\end{displaymath}
\begin{displaymath}
DNF((\neg a, c),(f,g))
\end{displaymath}
\begin{displaymath}
d = 0, f = 0
\end{displaymath}
A conflict occurs between these two constraints because $d$ and $f$ turn false. This conflict can be resolved by learning the following DNF constraint:
\begin{displaymath}
(d \wedge e) \vee (f \wedge g)
\end{displaymath}
\end{leftbar}
\caption{Example of a DNF-DNF conflict resolution}
\label{ex:DNFDNFConflictResolution}
\end{example}

The Example \ref{ex:DNFDNFConflictResolution} shows a simple DNF-DNF conflict, that can be resolved by the Lemma \ref{le:DNFDNFConflict}, that we defined. Now consider the following conversion of the learned DNF constraint as a CNF

\begin{leftbar}
\begin{displaymath}
(d \vee f) \wedge (d \vee g) \wedge (e \vee f) \wedge (e \vee g)
\end{displaymath}
\end{leftbar}

So by learning the new DNF constraint, the algorithm effectively learns four new clauses. This number could grow exponentially with the size of the DNF constraint. A more effective approach, would be to learn a single clause, in order to resolve a DNF-DNF conflict. For this consider the following lemma:

\begin{lemma}
\begin{leftbar}
Consider two DNF constraints $DNF_1$ and $DNF_2$ and the variable $x$, that the two constraints are complementary on. $DNF_1$ contains the literal $x$ and $DNF_2$ contains the literal $\neg x$. Let $\{y_1,...,y_n\}$ be the literals of $DNF_1$, that turned false and therefore forced a unit propagation of the literal $x$. Let $\{z_1,...,z_m\}$ be the literals of $DNF_2$, that turned false and therefore caused the propagation of the literal $\neg x$. Then this specific conflict can be resolved by learning the following clause:
\begin{displaymath}
y_1 \vee ... \vee y_n \vee z_1 \vee ... \vee z_m
\end{displaymath}
\end{leftbar}
\label{le:DNFDNFConflictAlt}
\end{lemma}

\begin{proof}
Because there is a conflict between $DNF_1$ and $DNF_2$, the current partial variable assignment can't satisfy both constraints. Because only false literals are responsible for non satisfying assignments in DNF constraints, only these literals are responsible for the conflict. That means that at least one of the literals $\{y_1,...,y_n\}$ or $\{z_1,...,z_m\}$ needs to be true, so that a satisfying assignment can be reached. This can be expressed as the clause $y_1 \vee ... \vee y_n \vee z_1 \vee ... \vee z_m$.
\end{proof}

\subsection{DNF - Clause Conflict}

This section contains the conflict resolution between DNF constraints and clauses. First consider the following example:

\begin{example}
\begin{leftbar}
\begin{displaymath}
DNF((a,b,c),(e,f,g))
\end{displaymath}
\begin{displaymath}
Clause(c,d,\neg e)
\end{displaymath}
\begin{displaymath}
c = 0, d = 0
\end{displaymath}
\end{leftbar}
\caption{Example of a DNF-Clause conflict}
\label{ex:DNFClauseConflict}
\end{example}

The Example \ref{ex:DNFClauseConflict} shows a conflict between a DNF constraint and a clause. Because the literals $\{c,d\}$ are already false, the literal $e$ and $\neg e$ need to be true at the same time, which causes a conflict. This conflict can be resolved by applying the following lemma:



\begin{lemma}
\begin{leftbar}
Consider a DNF constraint and a clause, that are complementary on the variable $x$. The DNF constraint contains the literal $x$ and the clause the literal $\neg x$. A conflict, that is caused by the variable $x$, can be resolved by learning the following constraint
\begin{displaymath}
DNF / x \vee Clause / \neg x
\end{displaymath}
\end{leftbar}
\label{le:DNFClauseConflict}
\end{lemma}

\begin{proof}
The proof for this lemma is simple, because a clause is just a special case of a DNF constraints. A clause is a disjunction of literals, which can be written as a DNF constraint, where each term has a single literal. This also means, that the proof of Lemma \ref{le:DNFDNFConflict} is also true for Lemma \ref{le:DNFClauseConflict}.
\end{proof}

By applying the Lemma \ref{le:DNFClauseConflict} to the Example \ref{ex:DNFClauseConflict}, the conflict can be resolved by learning the following constraint:

\begin{leftbar}
\begin{displaymath}
DNF((a,b,c),(c),(d))
\end{displaymath}
\end{leftbar}

\subsubsection{Alternative DNF-Clause Conflict Resolution Rule}

The DNF-Clause conflict resolution rule has a similar problem to the DNF-DNF conflict resolution, because in both cases the algorithm has to learn a DNF constraint. As already explained before, a DNF constraint is difficult to handle, because the algorithm effectively learns several clauses at each conflict, which makes the propagation significantly harder after each conflict. That is why it is beneficial, to also only learn a single clause, in order to resolve the conflict.

\begin{lemma}
\begin{leftbar}
Consider a DNF constraint and a clause. Let $x$ be a variable, that both constraints are complementary on. The DNF constraint contains the literal $x$ and the clause contains the literal $\neg x$. Let $\{y_1,...,y_n\}$ be the literals of the DNF constraint, that turned false and therefore forced a unit propagation of the literal $x$. Let $\{z_1,...,z_m\}$ be the literals of the clause, that turned false and therefore caused the propagation of the literal $\neg x$. Then this specific conflict can be resolved by learning the following clause:
\begin{displaymath}
y_1 \vee ... \vee y_n \vee z_1 \vee ... \vee z_m
\end{displaymath}
\end{leftbar}
\label{le:DNFClauseConflictAlt}
\end{lemma}


\begin{proof}
Because the clause is just a special case of a DNF constraint, the same proof as for Lemma \ref{le:DNFDNFConflictAlt} is also valid here for the Lemma \ref{le:DNFClauseConflictAlt}.
\end{proof}



\subsection{DNF - AMO Conflict}

This section contains the conflict resolution between DNF constraints and AMO constraints. First consider the following example:

\begin{example}
\begin{leftbar}
\begin{displaymath}
DNF((b,c),(d,e))
\end{displaymath}
\begin{displaymath}
AMO(a,b,c)
\end{displaymath}
\begin{displaymath}
a = 1, d = 0, e = 0
\end{displaymath}
\end{leftbar}
\caption{Example for a conflict between a DNF constraint and an AMO constraint}
\label{ex:AMODNFConflict}
\end{example}


The Example \ref{ex:AMODNFConflict} shows a conflict between an AMO constraint and a DNF constraint. Because the literal $a$ is true, the AMO constraint forces a unit propagation of $\neg b$ and $\neg c$. But because the literals $\{d,e\}$ are already false, $b$ and $c$ need to be true, in order to satisfy the DNF constraint. This causes a conflict. It can be resolved by applying the following lemma:


\begin{lemma}
\begin{leftbar}
Consider a DNF constraint and an AMO constraint. Let $x$ be a literal, that is contained in both constraints. Let $\{z_1,...,z_m\}$ be the set of literals in the AMO constraint except for $x$. A conflict caused by the literal $x$ can then be resolved by learning the following constraint.
\begin{displaymath}
DNF / x \vee (\neg z_1 \wedge ... \wedge \neg z_m)
\end{displaymath}
\end{leftbar}
\label{le:AMODNFConflict}
\end{lemma}

\begin{proof}
Consider the literal $x$, that is contained in both constraints. If any literal besides $x$ turns true in the AMO constraint, $x$ automatically has to be false. This can cause a conflict when every non-false term in the DNF constraint contains the literal $x$. In order to resolve this conflict either any term, that doesn't contain $x$ needs to be true, or every literal besides $x$ needs to be false in the AMO constraint. This can be achieved by learning the constraint $DNF / x \vee (\neg z_1 \wedge ... \wedge \neg z_m)$.
\end{proof}

\subsubsection{Alternative DNF-AMO Conflict Resolution}

The resolution rule for DNF-AMO conflicts again results in at least one DNF constraint, that is learned. Here it is also preferable, to instead learn a single clause, to resolve the conflict. This can be achieved with the following rule:


\begin{lemma}
\begin{leftbar}
Consider a DNF constraint and an AMO constraint. Let $x$ be the variable, that the constraints are conflicting on. Let $y$ be the other literal of the AMO constraint that is true. Let $\{z_1,...,z_m\}$ be the set of literals of the DNF constraint, that turned false and therefore led to this conflict. Then this conflict can be resolved by learning the following clause
\begin{displaymath}
\neg y \vee z_1 \vee ... \vee z_m
\end{displaymath}
\end{leftbar}
\end{lemma}

\begin{proof}
The conflict is on the literal $x$. The conflict can only occur, if there is another literal $y$ in the AMO constraint, that is currently true. So in order for this specific conflict to resolve either $y$ has to be false, so that the AMO constraint can be satisfied, or one of the literals $\{z_1,...,z_m\}$, that forced $x$ to be a unit literal, needs to be true.
\end{proof}

\chapter{Implementation}
\label{ch:Implementation}

This chapter discusses the implementation details of our novel SAT Solver architecture. As already explained in Chapter \ref{ch:Analysis}, the goal of this thesis is the implementation of a novel SAT Solver architecture, that is both backwards compatible with current SAT solver implementations by using the DIMACS file format, while also being capable of processing new constraints, that aren't represented in the standard CNF format. In order to make this possible, we first had to define a new format, that can represent the additional constraints. The complete architecture was programmed in Java.

\section{Rich CNF Format (RCNF)}

We introduce a new format called the "Rich CNF Format (RCNF)" which is capable of representing formulas in the CNF format with additional DNF and AMO constraints. The format is an extension of the already mentioned DIMACS file format for representing CNF formulas.

\begin{leftbar}
The format for representing AMO constraints is similar to the representation of clauses. The only difference is the identifier string "AMO" in the beginning:
\begin{displaymath}
AMO \; x_1 \; ... \; x_n \; 0
\end{displaymath}
\end{leftbar}

In contrast to the AMO constraints and clauses, the DNF constraint can't be represented by just a list of literals. In a DNF constraint it is important to clearly signify, which term a literal belongs to. Otherwise the representation could be misinterpreted, which could lead to wrong results.

\begin{leftbar}
The DNF constraint representation starts with the identifier string "DNF", which is then followed by a list of literals. Each term for the DNF constraint is separated by a 0:
\begin{displaymath}
DNF \; x_1 \; ... \; x_n \; 0 \; y_1 \; ... \; y_n \; 0 \; 0
\end{displaymath}
\end{leftbar}

Each DNF constraint ends with two "0" characters. The last "0" character is used as a line termination like in the original DIMACS format, while the second to last "0" character is used to signal the end of the last term.

\begin{center}
\begin{leftbar}
\begin{tabular}{l}
\texttt{p cnf 10 3} \\
\texttt{1 2 -3 0} \\
\texttt{DNF 4 5 0 6 7 -8 0 9 -10 0 0}\\
\texttt{AMO 3 4 -5 6 0} \\
\end{tabular}
\end{leftbar}
\captionof{example}{Example of a DIMACS RCNF-file}
\label{ex:RCNF}
\end{center}

The Example \ref{ex:RCNF} shows an input file in the defined RCNF format. The first line describes how many variables and constraints the input formula consists of. In this example there are ten variables and 3 constraints. The number of constraints is the sum of all clauses, DNF constraints and AMO constraints.

\section{The Constraint Implementation}

As already described in Chapter \ref{ch:Analysis}, an important part of this thesis is the conflict resolution between the different types of constraints. The current modern SAT solvers just need to be able to resolve conflicts between clauses. This allows for a very efficient implementation with arrays, because there is no need for differentiation between different constraint types. In our novel architecture the solver needs to be able to clearly differentiate, between which types of constraint a conflict occurred, in order to be able to apply the different rules.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{Classes.pdf}
  \caption{The constraint classes used by the SAT solver}
  \label{fig:constraints}
\end{figure}

Figure \ref{fig:constraints} shows the class diagram of the constraints, that are used in the solver. All the constraints in our implementation inherit from the abstract class "Constraint". This superclass allows for a general implementation of the solving algorithm, that treats every constraint the same. Of course the propagation of a literal is very different depending on the type of constraint. In order to ensure a correct propagation, every subclass of "Constraint" implements its own "propagate()" method.

\subsection{The "DisjunctiveConstraint" Class}

This class contains the implementation of the clause constraints, as the name already suggests. Internally every literal of the clause is kept inside a primitive integer array. Simple arrays have the advantage of minimal memory consumption and very fast access times. The propagate() method makes use of the Two-Watched-Literals algorithm, that was already described in Chapter \ref{ch:Preliminaries}. That means, that there are always two non false literals watched. If any of them turn false, then another non false literal is set as a replacement. If there are no replacement literals left, then the last non false watched literal must be propagated as a unit literal.

\subsection{The "BinaryDisjunctiveConstraint" Class}

This is a subclass of the "DisjunctiveConstraint" class. The DisjunctiveConstraint class is already not very computationally expensive, but we considered, that an improvement might be possible by dedicating a class to the special case of binary clauses. These binary clauses have the advantage, that if any of the two literals turns false, the other can be instantly propagated. We take advantage of that in the "BinaryDisjunctiveConstraint" class.

\subsection{The "AMOConstraint" Class}

This class contains the implementation of the "At Most One" constraint. AMO constraints are similar to clauses, because they can be fully described by just a list of literals.
The literals of our AMO constraints are also kept in a primitive integer array, similar to the "DisjunctiveConstraint" class. The propagate() method also makes use of watched literals. Contrary to the clauses, every literal in the AMO constraint needs to be watched. This is necessary, because if just a single literal turns true, every other literal needs to be false. So every true literal in an AMO constraint instantly triggers a unit propagation.

\subsection{The "DNFConstraint" Class}

This class contains the implementation of the "DNF" constraint. In contrast to the clause and AMO constraints, a DNF constraint can't be simply represented by a list of literals. Internally the class uses an array of primitive integer arrays, in order to represent the different terms. Here the minimal memory consumption and fast access speed are the reason, arrays are used. The literal propagation is far more complex in DNF constraints, than it is in clauses and AMO constraints. That is the case, because a unit propagation needs to start, if all non false terms intersect on a literal. This makes it difficult to find an algorithm, that is similar to the Two-Watched-Literals algorithm for clauses. The DNFConstraint class therefore uses the algorithm, that was already defined in Chapter \ref{ch:Analysis}.
\par
Because the intersection between terms is very important for the propagation algorithm, the class pre calculates a list for every literal, that contains all terms, that share this literal. These lists can then be efficiently used, to check for intersections.

\subsubsection{The "BinaryDNFConstraint" Class}

This is a subclass of the DNFConstraint class. During development we recognized, that the DNF constraints need a lot of computation time, because of the many intersection calculations, that need to be done in order to ensure propagation completeness. In the case of binary DNF constraints these calculations are not necessary. That's why the BinaryDNFConstraint class uses simpler techniques. A binary DNF constraint contains only two terms. During the class initialization we check, if there are any shared literals between these two terms. These literals need to be unit literals. After that the algorithm just watches the two terms of the binary DNF constraint. If one of the terms turns false, the other needs to be instantly propagated. By using this class computational time and memory can be saved, when the formula has many binary DNF constraints.

There is also the special case of a DNF constraint with only one term. We didn't create a dedicated class for this case, but it was considered in the implementation of the DNFConstraint class. There the algorithm checks, if the constraint has only one term. If that is true, then every literal of this term gets instantly propagated.

\section{The "Formula" Class}

This class is the centerpiece of the architecture and contains the whole formula, as the name suggests. It is needed to keep track of variable assignments, unit literals, phase saving states, constraints, watched literals and also conflicts.
\paragraph{The "variables" array} is needed, in order to keep track of the current variable assignments. It contains an entry for each existing variable. These entries are either "0", "-1" or "1", which mean "no assignment", "false" or "true" respectively.
\paragraph{The "unitLiteralState" array} is similar to the variables array and has an entry for each existing variable. It keeps track, if a literal needs to be propagated as a unit literal. The entries are similar to the variables array.
\paragraph{The "phaseSavingLastAssignment" array} also has an entry for each existing variable and keeps track of the last assignment of this variable. This is used to implement the phase saving technique.
\par
An important part of the "Formula" class are the lists, that keep track of in which constraint a literal is watched. The "Formula" class contains two arrays for each type of constraint. The first array keeps track of the constraints, where the variables are positively watched, and the second array keeps track of the constraints, where the variables are negatively watched. Each of these arrays has an entry for each variable. These entries are lists of constraints, where the literal is watched.
\par
The most important task of the "Formula" class is the delegation of the literal propagation to the correct constraints. When a literal gets propagated, then the class selects the correct lists to see, where the literal is watched. It then invokes the propagate() method on every constraint, that is contained in these lists. During the CDCL-algorithm some learned constraints can be declared obsolete, in order to reduce the database size. If such a constraint is encountered, then it is removed. If a conflict occurs during the propagation in one of the constraints, then the class also saves the literal and constraint, that caused the conflict.

\section{The "SolverAlgorithm" Class}

This class contains the implementation of the general solving procedure for the formulas. In an early concept this class was supposed to have several subclasses, that each contain different algorithms for SAT solving. Later on this class was re purposed to just hold the general structure of a solving algorithm. This generic algorithm calls several abstract strategy classes, that change the course of the algorithm, depending on which of their subclasses is used by the dynamic binding of Java. Specifically these classes use the strategy pattern for the conflict handling, the restart scheduling and the variable selection.

\begin{algorithm}
\caption{solve(formula)}\label{alg:cap}
\begin{algorithmic}
\State $trail \gets \{\}$
\While{$trail.size() \neq formula.numberOfVariables()$}
	\State $unitPropagation()$
\If{$formula.hasConflict()$}
	\State $conflictHandlingStrategy()$
	\State $restartHandlingStrategy()$
\Else
    \State $x \gets variableSelectionStrategy()$
    \State $formula.propagate(x)$
    \State $trail.assign(x)$
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

The algorithm \ref{alg:cap} shows the general solver algorithm, that our novel SAT solver architecture uses. There are several abstract strategies, that change the course of the algorithm, depending on which of their subclasses gets invoked.

\section{The "ConflictHandlingStrategy" Class}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=1.2]{ConflictHandlingStrategy.pdf}	
  \caption{The conflict handling strategies used by the solver}
  \label{fig:conflictHandling}
\end{figure}

Figure \ref{fig:conflictHandling} shows the ConflictHandlingStrategy class and its two subclasses. The two subclasses both have a different approaches to handling the conflicts, that occur during the solving process. There is the "DPLLConflictHandler", that uses the conflict handling strategy of the DPLL algorithm, in order to resolve conflicts. And there is the "CDCLConflictHandler" that uses Conflict-Driven-Clause-Learning, in order to resolve the conflicts
\par
The DPLLConflictHandler uses a simple procedure, independent of which type of conflict occurs during the solving process. If a conflict occurs in the algorithm, then this class unassigns every variable on the trail until it finds a variable, that wasn't assigned by unit propagation and hasn't been assigned both "true" and "false" as values. This variable then gets propagated with the value, that hasn't been tried yet. This procedure gets repeated until every combination of variable assignments have been tried.
\par
The CDCLConflictHandler makes use of our modified version of the CDCL-algorithm. If a conflict occurs between two clauses, then the algorithm just follows the normal procedure of the already described CDCL-algorithm. If a conflict occurs between two other constraints, then the class determines the constraint, that the conflict occurred in and calls its "handleConflict()" method. This call gives the conflict constraint the responsibility on how to handle the conflict.

\subsection{The Conflict Handling}

Each constraint type has an implementation of the "handleConflict()" method. This method always gets called on the constraint, in which the conflict occurred in. In this method the conflict constraint determines the constraint, that is the reason for this conflict. The "Formula" class keeps track of, which constraint caused the propagation of a unit literal. We call these constraints the "reason constraint" of the unit literal. The conflict constraint therefore determines the reason constraint of the conflict literal. Now depending on which type of constraints are involved in the conflict, the algorithm applies the conflict resolution rules that were determined in Chapter \ref{ch:Analysis} and derive the learnable constraints. We decided to only use the alternative conflict resolution rules.

\subsection{The Backtracking Process}

After the learn-able constraints are determined, the CDCLConflictHandler first checks, if the set of learned constraints is either empty, or contains a constraint, that is empty. In this case the formula is unsatisfiable and the algorithm stops.
\par
In order for the learned constraint to have an effect, the algorithm needs to backtrack the trail enough, so that each learned constraint is at least still satisfiable. The necessary decision level to ensure this condition, is the minimum decision level over all needed decision levels of all learned constraints. Each constraint type has a "getNeededDecisionLevel()" method, that determines the necessary level depending on the constraint type. The algorithm backtracks to the determined decision level by unassigning all variables on the trail, until the needed level is reached.

\subsection{Constraint Database Reduction}

The standard CDCL-algorithm and our derived version have in common, that a long solving process results in a large amount of constraints, that are learned. The constraint database will reach a point, where the advantage of the learned constraints is overshadowed by the overhead of memory and propagation time, that is needed to visit them. Therefore we implemented the constraint database reduction algorithm the way it is described in the paper "Predicting learnt clauses quality" \cite{audemard2009predicting} by Audemard and Simon. If the number of conflicts reaches 20000 then the database gets halved. With every database reduction this threshold gets elevated by 500. Before halving the database, we sort it according to the LBD scores of the learned constraints.

\section{The "RestartSchedulingStrategy" Class}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=1.2]{RestartSchedulingStrategy.pdf}	
  \caption{The restart strategies used by the solver}
  \label{fig:restartScheduling}
\end{figure}


Figure \ref{fig:restartScheduling} shows the RestartSchedulingStrategy class and its two subclasses. The two subclasses contain different implementations on how to handle restarts during the solving process. The "NoRestartsSchedulingStrategy" class is a simple stub class, that just never restarts the solving process. This is necessary, if the user doesn't want any restarts. The "ReluctantDoublingRestartStrategy" class makes use of the static restarting algorithm based on the Luby sequence. In order to ensure an efficient implementation, it uses the reluctant doubling algorithm, in order to calculate the sequence \cite{PAAR-2012:Practical_Aspects_of_SAT, biere_practical_nodate}.

\section{The "VariableSelectionStrategy" Class}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=1.2]{VariableSelectionStrategy.pdf}
  \caption{The different variable selection strategies used by the solver}
  \label{fig:variableSelection}
\end{figure}

Figure \ref{fig:variableSelection} shows the abstract class "VariableSelectionStrategy" and its two subclasses. The subclasses contain different implementations of decision heuristics, that determine the next variable that gets assigned.

The "FirstOpenVariableSelection" class contains a very simple implementation. It just iterates over the variables array of the "Formula" class and determines the next variable, by just choosing the first variable, that currently has no assignment.

The "VSIDS" class contains the implementation of the exponential VSIDS (EVSIDS) decision heuristic. In order to use this heuristic, the "Formula" class keeps an array of variable scores. This array has an entry for each variable in the formula, and is initialized with the number of variable occurences. Later on this occurrence count will be adjusted and used as a general score. During a conflict the algorithm keeps track of, which variables are involved. After that the score of these variables gets adjusted by using the following formula, which was suggested in the paper "Glucose in the SAT 2014 competition" \cite{audemard2014glucose} by Audemard and Simon:
\begin{displaymath}
score = score + (\frac{1}{0.8 + (0.01 * (conflictIndex \; div \; 5000))})^{conflictIndex}
\end{displaymath}
In this case the $div$ stands for an integer division, which effectively means, that the 0.8 increases by 0.01 every 5000th conflict. When the fraction reaches a value of 1.05 the algorithm stops using the formula and uses 1.05 as a constant.
In the "VSIDS" class the solver keeps a priority queue of every variable, sorted by their score. If the next variable needs to be selected, the priority queue gets polled until it draws a variable, that is currently unassigned. This variable is then chosen as the next variable, that gets propagated. During the backtracking process after a conflict, the priority queue gets filled with every variable, that gets unassigned.

\section{The "IncrementalCDCLSolver" Class}

\begin{figure}[H]
  \centering
  \includegraphics[scale=1.2]{IncrementalSatSolver.pdf}
  \caption{The incremental SAT solver interface and its subclass}
  \label{fig:incrementalSAT}
\end{figure}

Figure \ref{fig:incrementalSAT} shows the IncrementalCDCLSolver class. The class inherits from the "IterativeSolvingAlgorithm" class, so that it can make use of the already implemented "solve()" method. It then also implements the "IncrementalSatSolver" interface, which offers the methods to add clauses, AMO constraints and DNF constraints directly in the code instead of having to work with the "*.cnf" or "*.rcnf" files on the hard drives. This is especially useful while doing experiments, that constantly change the number of variables or the amount of constraints.
